{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "collapsed_sections": [
        "gqjRsfxH891x",
        "J_wIalJJs0ki"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Multimodal Structured Outputs with Daft, Gemma-3n, and vLLM\n",
        "\n",
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/everettVT/daft-structured-outputs/blob/main/workload/Daft_Structured_Outputs_Gemma3_vLLM.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>\n",
        "\n",
        "\n",
        "This notebook walks through a practical example of evaluating model performance on structured generation and multimodal reasoning. We will explore how to scale multimodal image understanding evaluation using a combination of powerful technologies:\n",
        "1. Daft for data processing\n",
        "2. Gemma-3n-E2B model for multimodal capabilities\n",
        "3. vLLM/OpenRouter for efficient inference serving.\n",
        "\n",
        "---\n",
        "\n",
        "### Table of Contents\n",
        "1. Setup and Install Dependencies\n",
        "2. Launch a vLLM OpenAI API compatible server\n",
        "3. Testing the OpenAI client Gemma-3n client with an API key and new base url.\n",
        "4. Preprocess the ai2d dataset from huggingface's Cauldron collection\n",
        "5. Multimodal Inference with Structured Outputs using 3 approaches\n",
        "6. Analysis of the inference results\n",
        "7. Putting everything together: Evaluating Gemma-3n-e4b-it across the AI2D subset\n",
        "8. Conclusion\n",
        "9. Appendix\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "A8n0CLwwb5dZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## First, install  dependencies\n",
        "You will be prompted to restart the session following installation, which simply clears local variables. If you ever need to kill the session or restart further into the notebook, you will not need to reinstall dependencies or authenticate with HuggingFace."
      ],
      "metadata": {
        "id": "E8P5OPUCtIkO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install \"daft[huggingface]==0.5.22\" vllm"
      ],
      "metadata": {
        "id": "QZuYcV6s7R43"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " #### Login to HF for access gemma-3n"
      ],
      "metadata": {
        "id": "HXBgajW4ssGZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!hf auth login"
      ],
      "metadata": {
        "id": "JIC6JtSPseqB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# Online Serving - Launch vLLM OpenAI Compatible Server\n",
        "\n",
        "Run the following vllm cli command in your terminal\n",
        "\n",
        "It usually takes at least **7.5** minutes before the vLLM server is ready\n",
        "\n",
        "If you are in Google Colab, you can open a terminal by clicking the terminal icon in the bottom left of the ui.\n",
        "```bash\n",
        " python -m vllm.entrypoints.openai.api_server \\\n",
        "  --model google/gemma-3n-e4b-it \\\n",
        "  --enable-chunked-prefill \\\n",
        "  --guided-decoding-backend guidance \\\n",
        "  --dtype bfloat16 \\\n",
        "  --gpu-memory-utilization 0.85 \\\n",
        "  --host 0.0.0.0 --port 8000\n",
        "```\n",
        "\n",
        "----\n",
        "\n",
        "### NOTE: The memory allocation for vLLM is optimized for google colab's A100 instance and gemma-3n-e4b-it. Using a larger model will take up more GPU RAM and you will need to allocate less utilization.\n",
        "\n"
      ],
      "metadata": {
        "id": "mtIxKYwIrjG7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Verify you can connect to vLLM Online Serving using OpenAI Client"
      ],
      "metadata": {
        "id": "gqjRsfxH891x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "api_key = \"none\"\n",
        "base_url = \"http://0.0.0.0:8000/v1\"\n",
        "model_id = 'google/gemma-3n-e4b-it'\n",
        "client = OpenAI(api_key=api_key, base_url=base_url)"
      ],
      "metadata": {
        "id": "jKoQGBfQ8zqi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test Client model list contains `google/gemma-3n-e4b-it`\n",
        "result = client.models.list()\n",
        "\n",
        "print(result)"
      ],
      "metadata": {
        "id": "0yL-XBvtFv0Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test Simple Text Completion\n",
        "chat_completion = client.chat.completions.create(\n",
        "    messages=[{\"role\": \"user\", \"content\": \"What is the capital of the United States?\"}],\n",
        "    model=model_id,\n",
        ")\n",
        "\n",
        "result = chat_completion.choices[0].message.content\n",
        "print(\"Chat completion output: \\n\", result)"
      ],
      "metadata": {
        "id": "FmywS9kiTZLO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test Structured Output\n",
        "completion = client.chat.completions.create(\n",
        "    model=model_id,\n",
        "    messages=[\n",
        "        {\"role\": \"user\", \"content\": \"Classify this sentiment: Daft is wicked fast!\"}\n",
        "    ],\n",
        "    extra_body={\"guided_choice\": [\"positive\", \"negative\"]},\n",
        ")\n",
        "print(completion.choices[0].message.content)"
      ],
      "metadata": {
        "id": "071agIW4Qg4n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test Image Understanding\n",
        "image_url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/bee.jpg\"\n",
        "completion = client.chat.completions.create(\n",
        "    model=model_id,\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": [{\"type\": \"text\", \"text\": \"You are a helpful assistant.\"}]\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                {\"type\": \"image_url\", \"image_url\": {\"url\": image_url}},\n",
        "                {\"type\": \"text\", \"text\": \"Describe this image in detail.\"}\n",
        "            ]\n",
        "        }\n",
        "    ]\n",
        ")\n",
        "print(completion.choices[0].message.content)"
      ],
      "metadata": {
        "id": "rjj5_JFQczgt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test Combining Image Inputs with Structured Output\n",
        "\n",
        "We can play with prompting/structured outputs to understand how prompting and structured outputs can affect results.\n",
        "\n",
        "Try commenting out the `extra_body` argument or the third user content text prompt to see how results change."
      ],
      "metadata": {
        "id": "osXVHWQ7eHim"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "completion = client.chat.completions.create(\n",
        "    model=model_id,\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": [{\"type\": \"text\", \"text\": \"You are a helpful assistant.\"}]\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                {\"type\": \"image_url\", \"image_url\": {\"url\":image_url}},\n",
        "                {\"type\": \"text\", \"text\": \"Which insect is portrayed in the image: A. Ladybug, B. Beetle, C. Bee, D. Wasp \"},\n",
        "                #{\"type\": \"text\", \"text\": \"Answer with only the letter from the multiple choice. \"} # Try comment me out\n",
        "            ]\n",
        "        }\n",
        "    ],\n",
        "    extra_body={\"guided_choice\": [\"A\", \"B\", \"C\", \"D\"]}, # Try comment out\n",
        "\n",
        ")\n",
        "print(completion.choices[0].message.content)"
      ],
      "metadata": {
        "id": "Ril6tbiTeLn4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "#  Dataset Preprocessing"
      ],
      "metadata": {
        "id": "UhgBWDkm9RVy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prepping the [HuggingFaceM4/the_cauldron](https://huggingface.co/datasets/HuggingFaceM4/the_cauldron/viewer?views%5B%5D=ai2d) ,  Dataset for inference (ai2d subset)\n",
        "\n",
        "We can read directly from huggingface datasets by leveraging the `hf://` prefix in the url string."
      ],
      "metadata": {
        "id": "MYfIxkMxf8Nc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import daft\n",
        "\n",
        "# There are a total of 2,434 images in this dataset, at a size of ~ 500 MB\n",
        "df_raw = daft.read_parquet('hf://datasets/HuggingFaceM4/the_cauldron/ai2d/train-00000-of-00001-2ce340398c113b79.parquet').collect()\n",
        "df_raw.show(3)"
      ],
      "metadata": {
        "id": "27J-inULb4kn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Taking a look at the schema we can see the familiar messages nested datatype we are used to in chat completions inside the `texts` column\n"
      ],
      "metadata": {
        "id": "NEVwwhXDPohX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_raw.schema())"
      ],
      "metadata": {
        "id": "IZHGfSd_74K9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets decode the image bytes to see a preview of the images and add one more column for the base64 encoding. You can click on a cell to have a preview pop up."
      ],
      "metadata": {
        "id": "lpxeWOz8P4pO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from daft import col\n",
        "from daft import DataType\n",
        "import base64\n",
        "\n",
        "df = df_raw.explode(col(\"images\")).with_columns({\n",
        "    \"image_png\":    df_raw[\"images\"].struct.get(\"bytes\").image.decode(),\n",
        "    \"image_base64\": df_raw[\"images\"].struct.get(\"bytes\").apply(\n",
        "        lambda x: base64.b64encode(x).decode('utf-8'),\n",
        "        return_dtype=daft.DataType.string()\n",
        "    ),\n",
        "})\n",
        "df.show(3)"
      ],
      "metadata": {
        "id": "zFmi01VkPmUA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Preprocessing the 'texts' column to extract Question, Choices, and Answer Columns\n",
        "\n",
        "Copy/Pasting an entry from the `texts` column yields an openai messages list of dicts of the form:\n",
        "\n",
        "```python\n",
        "[{\n",
        "    \"user\": \"\"\"Question:\n",
        "            \n",
        "        From the above food web diagram, what cause kingfisher to increase\n",
        "\n",
        "        Choices:\n",
        "            A. decrease in fish\n",
        "            B. decrease in water boatman\n",
        "            C. increase in fish\n",
        "            D. increase in algae\n",
        "\n",
        "        Answer with the letter.\"\"\",\n",
        "\n",
        "    \"assistant\": \"Answer: C\",\n",
        "    \"source\": \"AI2D\",\n",
        "}, ...]\n",
        "```"
      ],
      "metadata": {
        "id": "KJFwixJpQTAn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Explode the List of Dicts inside \"texts\" to extract \"user\" and \"assistant\" messages\n",
        "df = df.explode(col(\"texts\")).collect()\n",
        "\n",
        "# Extract User and Assistant Messages\n",
        "df = df.with_columns({\n",
        "    \"user\": df[\"texts\"].struct.get(\"user\"),\n",
        "    \"assistant\": df[\"texts\"].struct.get(\"assistant\")\n",
        "}).collect()\n",
        "df.show(3)"
      ],
      "metadata": {
        "id": "HMW536eHUJ6Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also go above an beyond to parse each text input into individual question, choices, and answer columns.  "
      ],
      "metadata": {
        "id": "brCTE8koRhfb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Parsing \"user\" and \"assistant\" messages for question, choices, and answer\"\"\n",
        "df_prepped = df.with_columns({\n",
        "    \"question\": df[\"user\"]\n",
        "        .str.extract(r\"(?s)Question:\\s*(.*?)\\s*Choices:\")\n",
        "        .str.replace(\"Choices:\", \"\")\n",
        "        .str.replace(\"Question:\",\"\"),\n",
        "    \"choices_string\": df[\"user\"]\n",
        "        .str.extract(r\"(?s)Choices:\\s*(.*?)\\s*Answer?\\.?\")\n",
        "        .str.replace(\"Choices:\\n\", \"\")\n",
        "        .str.replace(\"Answer\",\"\"),\n",
        "    \"answer\": df[\"assistant\"]\n",
        "        .str.extract(r\"Answer:\\s*(.*)$\")\n",
        "        .str.replace(\"Answer:\",\"\"),\n",
        "}).collect()\n",
        "\n",
        "df_prepped.show(3)"
      ],
      "metadata": {
        "id": "JUD64rVnkWYE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# Multimodal Inference with Structured Outputs\n",
        "Now we will move on to scaling our OpenAI client calls with Daft UDFs"
      ],
      "metadata": {
        "id": "OkVMvG9cg30_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lets set a row limit while we test a few possible approaches\n",
        "import asyncio\n",
        "from daft import col, lit\n",
        "from daft.functions import format\n",
        "from openai import OpenAI, AsyncOpenAI\n",
        "import time\n",
        "\n",
        "model_id = 'google/gemma-3n-e4b-it'\n",
        "api_key = \"none\"\n",
        "base_url = \"http://0.0.0.0:8000/v1\"\n",
        "client = AsyncOpenAI(api_key=api_key, base_url=base_url)\n",
        "row_limit = 10000"
      ],
      "metadata": {
        "id": "RUkXRwcJ99dL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Minimal Row-Wise UDF"
      ],
      "metadata": {
        "id": "Q7MGO3n-cZB3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@daft.func(return_dtype=daft.DataType.string())\n",
        "async def struct_output_rowwise(model_id: str, text_col: str, image_col: str, extra_body: dict | None = None) -> str:\n",
        "    client = OpenAI(api_key=api_key, base_url=base_url)\n",
        "    content = [{\"type\": \"text\", \"text\": text_col}]\n",
        "    if image_col:\n",
        "        content.append({\n",
        "            \"type\": \"image_url\",\n",
        "            \"image_url\": {\"url\": f\"data:image/png;base64,{image_col}\"},\n",
        "        })\n",
        "\n",
        "\n",
        "    result = client.chat.completions.create(\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": content\n",
        "            }\n",
        "        ],\n",
        "        model=model_id,\n",
        "        extra_body=extra_body,\n",
        "    )\n",
        "    return result.choices[0].message.content"
      ],
      "metadata": {
        "id": "j1hMVg3jbQw3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the Rowwise UDF\n",
        "start = time.time()\n",
        "df_rowwise_udf = df_prepped.with_column(\"result\", struct_output_rowwise(\n",
        "    model_id = model_id,\n",
        "    text_col = format(\"{} \\n {}\", col(\"question\"), col(\"choices_string\")),\n",
        "    image_col = col(\"image_base64\"),\n",
        "    extra_body={\"guided_choice\": [\"A\", \"B\", \"C\", \"D\"]}\n",
        ")).with_column(\"is_correct\", col(\"result\").str.lstrip().str.rstrip() == col(\"answer\").str.lstrip().str.rstrip()).limit(row_limit).collect()\n",
        "end = time.time()\n",
        "print(f\"Row- wise UDF - Processed {df_rowwise_udf.count_rows()} rows in {end-start} seconds, {df_rowwise_udf.count_rows()/(end-start)} rows/s\")"
      ],
      "metadata": {
        "id": "t4G_yjdNdayW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write down each of your runs here:\n",
        "- Row-wise UDF - Processed ..."
      ],
      "metadata": {
        "id": "bGrFoxmyheX4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Minimal Async Batch UDF"
      ],
      "metadata": {
        "id": "ILuaaIIbcbXG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@daft.udf(return_dtype=daft.DataType.string())\n",
        "def struct_output_batch(\n",
        "        model_id: str,\n",
        "        text_col: daft.Series,\n",
        "        image_col: daft.Series,\n",
        "        extra_body: dict | None = None\n",
        "    ) -> list[str]:\n",
        "\n",
        "\n",
        "    async def generate(model_id: str, text: str, image: str) -> str:\n",
        "\n",
        "        content = [{\"type\": \"text\", \"text\": text}]\n",
        "        if image:\n",
        "            content.append({\n",
        "                \"type\": \"image_url\",\n",
        "                \"image_url\": {\"url\": f\"data:image/png;base64,{image}\"},\n",
        "            })\n",
        "\n",
        "        result = await client.chat.completions.create(\n",
        "            messages=[\n",
        "                {\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": content\n",
        "                }\n",
        "            ],\n",
        "            model=model_id,\n",
        "            extra_body=extra_body,\n",
        "        )\n",
        "        return result.choices[0].message.content\n",
        "\n",
        "    texts = text_col.to_pylist()\n",
        "    images = image_col.to_pylist()\n",
        "\n",
        "    async def gather_completions() -> list[str]:\n",
        "        tasks = [generate(model_id, t,i) for t,i in zip(texts,images) ]\n",
        "        return await asyncio.gather(*tasks)\n",
        "\n",
        "    return asyncio.run(gather_completions())"
      ],
      "metadata": {
        "id": "2WaluSJaQS1l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Run the Batch UDF\n",
        "start = time.time()\n",
        "df_batch_udf = df_prepped.with_column(\"result\", struct_output_batch(\n",
        "    model_id = model_id,\n",
        "    text_col = format(\"{} \\n {}\", col(\"question\"), col(\"choices_string\")), # Prompt Template\n",
        "    image_col = col(\"image_base64\"),\n",
        "    extra_body={\"guided_choice\": [\"A\", \"B\", \"C\", \"D\"]}\n",
        ")).with_column(\"is_correct\", col(\"result\").str.lstrip().str.rstrip() == col(\"answer\").str.lstrip().str.rstrip()).limit(row_limit).collect()\n",
        "end = time.time()\n",
        "print(f\"Batch UDF - Processed {df_batch_udf.count_rows()} rows in {end-start} seconds, {df_batch_udf.count_rows()/(end-start)} rows/s\")"
      ],
      "metadata": {
        "id": "uoHeK1XccrT7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write down each of your runs here:\n",
        "- Batch UDF - Processed ..."
      ],
      "metadata": {
        "id": "0u4-JB8BhpWl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Try increasing the row_limit variable to 500, 1000, and 2000 rows.\n",
        "- What happens if you try to run the full dataset (7462 rows)?\n",
        "- How does row processing rate change when you increase the row_limit?\n",
        "- Do you run into any issues?\n"
      ],
      "metadata": {
        "id": "apneVbIRhw4N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Production UDF\n",
        "\n",
        "Here is what a more productionized version of our minimal user defined functions looks like."
      ],
      "metadata": {
        "id": "MB1Zn_nP8PPc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "concurrency = 4\n",
        "max_conn = 32"
      ],
      "metadata": {
        "id": "w9XdLVpw-8im"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Any\n",
        "\n",
        "@daft.udf(return_dtype=daft.DataType.string(), concurrency=concurrency, batch_size= batch_size)\n",
        "class StructuredOutputsProdUDF:\n",
        "    def __init__(self, base_url: str, api_key: str):\n",
        "        self.client = AsyncOpenAI(base_url=base_url, api_key=api_key)\n",
        "        try:\n",
        "            self.loop = asyncio.get_running_loop()\n",
        "        except RuntimeError:\n",
        "            self.loop = asyncio.new_event_loop()\n",
        "            asyncio.set_event_loop(self.loop)\n",
        "\n",
        "\n",
        "    def __call__(self,\n",
        "        model_id: str,\n",
        "        text_col: daft.Series,\n",
        "        image_col: daft.Series,\n",
        "        sampling_params: dict[str, Any] | None = None,\n",
        "        extra_body: dict[str, Any] | None = None\n",
        "        ) -> list[str]:\n",
        "\n",
        "        async def generate(text: str, image: str) -> str:\n",
        "                content = []\n",
        "                if image:\n",
        "                    content.append({\n",
        "                        \"type\": \"image_url\",\n",
        "                        \"image_url\": {\"url\": f\"data:image/png;base64,{image}\"},\n",
        "                    })\n",
        "                if text:\n",
        "                    content.append({\"type\": \"text\", \"text\": text})\n",
        "\n",
        "                result = await self.client.chat.completions.create(\n",
        "                    messages=[\n",
        "                        {\n",
        "                            \"role\": \"user\",\n",
        "                            \"content\": content # Dataset prefers image first\n",
        "                        }\n",
        "                    ],\n",
        "                    model=model_id,\n",
        "                    extra_body=extra_body,\n",
        "                    **sampling_params\n",
        "                )\n",
        "                return result.choices[0].message.content\n",
        "\n",
        "        async def infer_with_semaphore(t, i):\n",
        "            return await generate(t,i)\n",
        "\n",
        "        async def gather_completions(texts,images) -> list[str]:\n",
        "            tasks = [infer_with_semaphore(t,i) for t,i in zip(texts,images)]\n",
        "            return await asyncio.gather(*tasks)\n",
        "\n",
        "        texts = text_col.to_pylist()\n",
        "        images = image_col.to_pylist()\n",
        "\n",
        "        return self.loop.run_until_complete(gather_completions(texts,images))"
      ],
      "metadata": {
        "id": "qR7GdUOM7-MV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Production UDF\n",
        "start = time.time()\n",
        "df_prod_udf = df_prepped.with_column(\"result\", StructuredOutputsProdUDF.with_init_args(\n",
        "    base_url=base_url,\n",
        "    api_key=api_key,\n",
        ").with_concurrency(concurrency)(\n",
        "    model_id = model_id,\n",
        "    text_col = format(\"{} \\n {}\", col(\"question\"), col(\"choices_string\")), # Prompt Template\n",
        "    image_col = col(\"image_base64\"),\n",
        "    extra_body={\"guided_choice\": [\"A\", \"B\", \"C\", \"D\"]}\n",
        ")).with_column(\"is_correct\", col(\"result\").str.lstrip().str.rstrip() == col(\"answer\").str.lstrip().str.rstrip()).limit(row_limit).collect()\n",
        "end = time.time()\n",
        "print(f\"Prod UDF - Processed {df_prod_udf.count_rows()} rows in {end-start} seconds, {df_prod_udf.count_rows()/(end-start)} rows/s\")"
      ],
      "metadata": {
        "id": "eae4qPp-_hQI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "___\n",
        "# Analysis\n",
        "Evaluating Gemma-3's performance on image understanding by comparing structured output responses to the answer."
      ],
      "metadata": {
        "id": "OVgjnPjfkEcz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pass_fail_rate = df_prod_udf.where(col(\"is_correct\")).count_rows() / df_prod_udf.count_rows()\n",
        "print(f\"Pass/Fail Rate: {pass_fail_rate}\")"
      ],
      "metadata": {
        "id": "b_GdTLRm_vgx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# How does this compare without images?\n",
        "# Here we will use Daft's native inference function llm_generate\n",
        "from daft.functions import llm_generate\n",
        "start = time.time()\n",
        "df_prod_no_img = df_prepped.with_column(\"result\", llm_generate(\n",
        "    input_column = format(\"{} \\n {}\", col(\"question\"), col(\"choices_string\")), # Prompt Template\n",
        "    model = model_id,\n",
        "    extra_body={\"guided_choice\": [\"A\", \"B\", \"C\", \"D\"]},\n",
        "    api_key=api_key,\n",
        "    base_url=base_url,\n",
        "    provider = \"openai\"\n",
        ")).with_column(\"is_correct\", col(\"result\").str.lstrip().str.rstrip() == col(\"answer\").str.lstrip().str.rstrip()).collect()\n",
        "end = time.time()\n",
        "print(f\"llm_generate - Processed {df_prod_no_img.count_rows()} rows in {end-start} seconds,  {df_prod_no_img.count_rows()/(end-start)} rows/s\")"
      ],
      "metadata": {
        "id": "0nlo1hCzM1_l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pass_fail_rate_no_img = df_prod_no_img.where(col(\"is_correct\")).count_rows() / df_prod_no_img.count_rows()\n",
        "print(f\"Pass/Fail Rate: {pass_fail_rate}\")"
      ],
      "metadata": {
        "id": "d2EJnsabBZpj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# Putting everything together: Evaluating Gemma across the AI2D Dataset\n",
        "Now that we have walked through implementing this image understanding evaluation pipeline from end to end, lets put it all together so we can take full advantage of lazy evaluation and provide opportunities for future extensibility and re-use."
      ],
      "metadata": {
        "id": "eALCQifQPmfp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from typing import Any\n",
        "import asyncio\n",
        "import base64\n",
        "\n",
        "import daft\n",
        "from daft import col, lit\n",
        "from daft.functions import format\n",
        "from openai import AsyncOpenAI\n",
        "\n",
        "import logging\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "@daft.udf(return_dtype=daft.DataType.string(), concurrency=4)\n",
        "class StructuredOutputsProdUDF:\n",
        "    def __init__(self, base_url: str, api_key: str):\n",
        "        self.client = AsyncOpenAI(base_url=base_url, api_key=api_key)\n",
        "        try:\n",
        "            self.loop = asyncio.get_running_loop()\n",
        "        except RuntimeError:\n",
        "            self.loop = asyncio.new_event_loop()\n",
        "            asyncio.set_event_loop(self.loop)\n",
        "\n",
        "\n",
        "    def __call__(self,\n",
        "        model_id: str,\n",
        "        text_col: daft.Series,\n",
        "        image_col: daft.Series,\n",
        "        sampling_params: dict[str, Any] | None = None,\n",
        "        extra_body: dict[str, Any] | None = None\n",
        "        ):\n",
        "\n",
        "        async def generate(text: str, image: str) -> str:\n",
        "                content = []\n",
        "                if image:\n",
        "                    content.append({\n",
        "                        \"type\": \"image_url\",\n",
        "                        \"image_url\": {\"url\": f\"data:image/png;base64,{image}\"},\n",
        "                    })\n",
        "                if text:\n",
        "                    content.append({\"type\": \"text\", \"text\": text})\n",
        "\n",
        "                result = await self.client.chat.completions.create(\n",
        "                    messages=[\n",
        "                        {\n",
        "                            \"role\": \"user\",\n",
        "                            \"content\": content # Dataset prefers image first\n",
        "                        }\n",
        "                    ],\n",
        "                    model=model_id,\n",
        "                    extra_body=extra_body,\n",
        "                    **sampling_params\n",
        "                )\n",
        "                return result.choices[0].message.content\n",
        "\n",
        "        async def infer_with_semaphore(t, i):\n",
        "            return await generate(t,i)\n",
        "\n",
        "        async def gather_completions(texts,images) -> list[str]:\n",
        "            tasks = [infer_with_semaphore(t,i) for t,i in zip(texts,images)]\n",
        "            return await asyncio.gather(*tasks)\n",
        "\n",
        "        texts = text_col.to_pylist()\n",
        "        images = image_col.to_pylist()\n",
        "\n",
        "        return self.loop.run_until_complete(gather_completions(texts,images))\n",
        "\n",
        "class TheCauldronImageUnderstandingEvaluationPipeline:\n",
        "    def __init__(self, base_url: str, api_key: str):\n",
        "        self.base_url = base_url\n",
        "        self.api_key = api_key\n",
        "\n",
        "    def __call__(self,\n",
        "        model_id: str,\n",
        "        dataset_uri: str,\n",
        "        sampling_params: dict[str,Any] | None = None,\n",
        "        concurrency: int = 4,\n",
        "        row_limit: int | None = None,\n",
        "        is_eager: bool = False,\n",
        "    ) -> daft.DataFrame:\n",
        "        \"\"\"Executes dataset loading, preprocessing, inference, and post-processing.\n",
        "        Evalutation must be run seperately since it requires materialization.\n",
        "        \"\"\"\n",
        "\n",
        "        if is_eager:\n",
        "            # Load Dataset and Materialize\n",
        "            df = self.load_dataset(dataset_uri)\n",
        "            df = df.limit(row_limit) if row_limit else df\n",
        "            df = self._log_processing_time(df)\n",
        "\n",
        "            # Preprocess\n",
        "            df = self.preprocess(df)\n",
        "            df = self._log_processing_time(df)\n",
        "\n",
        "            # Perform Inference\n",
        "            df = self.infer(df, model_id, sampling_params)\n",
        "            df = self._log_processing_time(df)\n",
        "\n",
        "            # Post-Process\n",
        "            df = self.postprocess(df)\n",
        "            df = self._log_processing_time(df)\n",
        "        else:\n",
        "            df = self.load_dataset(dataset_uri)\n",
        "            df = self.preprocess(df)\n",
        "            df = self.infer(df, model_id, sampling_params)\n",
        "            df = self.postprocess(df)\n",
        "            df = df.limit(row_limit) if row_limit else df\n",
        "\n",
        "        return df\n",
        "\n",
        "    @staticmethod\n",
        "    def _log_processing_time(df: daft.DataFrame):\n",
        "        start = time.time()\n",
        "        df_materialized = df.collect()\n",
        "        end = time.time()\n",
        "        num_rows = df_materialized.count_rows()\n",
        "        logger.info(f\"Processed {num_rows} rows in {end-start} sec, {num_rows/(end-start)} rows/s\")\n",
        "        return df_materialized\n",
        "\n",
        "    def load_dataset(self, uri: str) -> daft.DataFrame:\n",
        "        return daft.read_parquet(uri)\n",
        "\n",
        "    def preprocess(self, df: daft.DataFrame) -> daft.DataFrame:\n",
        "\n",
        "        # Convert png image byte string to base64\n",
        "        df = df.explode(col(\"images\")).with_column(\"image_base64\", df_raw[\"images\"].struct.get(\"bytes\").apply(\n",
        "        lambda x: base64.b64encode(x).decode('utf-8'),\n",
        "        return_dtype=daft.DataType.string()\n",
        "            )\n",
        "        )\n",
        "\n",
        "        # Explode Lists of User Prompts and Assistant Answer Pairs\n",
        "        df = df.explode(col(\"texts\")).with_columns({\n",
        "            \"user\": df[\"texts\"].struct.get(\"user\"),\n",
        "            \"assistant\": df[\"texts\"].struct.get(\"assistant\")\n",
        "        })\n",
        "\n",
        "        # Parse the Question/Answer Strings\n",
        "        df = df.with_columns({\n",
        "            \"question\": df[\"user\"]\n",
        "                .str.extract(r\"(?s)Question:\\s*(.*?)\\s*Choices:\")\n",
        "                .str.replace(\"Choices:\", \"\")\n",
        "                .str.replace(\"Question:\",\"\"),\n",
        "            \"choices_string\": df[\"user\"]\n",
        "                .str.extract(r\"(?s)Choices:\\s*(.*?)\\s*Answer?\\.?\")\n",
        "                .str.replace(\"Choices:\\n\", \"\")\n",
        "                .str.replace(\"Answer\",\"\"),\n",
        "            \"answer\": df[\"assistant\"]\n",
        "                .str.extract(r\"Answer:\\s*(.*)$\")\n",
        "                .str.replace(\"Answer:\",\"\"),\n",
        "        })\n",
        "        return df\n",
        "\n",
        "    def infer(self,\n",
        "        df: daft.DataFrame,\n",
        "        model_id: str = 'google/gemma-3n-e4b-it',\n",
        "        sampling_params: dict[str,Any] = {\"temperature\": 0.0},\n",
        "        concurrency: int = 4,\n",
        "        extra_body: dict[str, Any] = {\"guided_choice\": [\"A\", \"B\", \"C\", \"D\"]}\n",
        "    ) -> daft.DataFrame:\n",
        "\n",
        "        return df.with_column(\"result\", StructuredOutputsProdUDF.with_init_args(\n",
        "            base_url=self.base_url,\n",
        "            api_key=self.api_key,\n",
        "        ).with_concurrency(concurrency)(\n",
        "            model_id = model_id,\n",
        "            text_col = format(\"{} \\n {}\", col(\"question\"), col(\"choices_string\")), # Prompt Template\n",
        "            image_col = col(\"image_base64\"),\n",
        "            sampling_params = sampling_params,\n",
        "            extra_body=extra_body\n",
        "        ))\n",
        "\n",
        "\n",
        "    def postprocess(self, df: daft.DataFrame) -> daft.DataFrame:\n",
        "        df = df.with_column(\"is_correct\", col(\"result\").str.lstrip().str.rstrip() == col(\"answer\").str.lstrip().str.rstrip())\n",
        "        return df\n",
        "\n",
        "    def evaluate(self, df: daft.DataFrame) -> float:\n",
        "        pass_fail_rate = df.where(col(\"is_correct\")).count_rows() / df.count_rows()\n",
        "        return pass_fail_rate"
      ],
      "metadata": {
        "id": "TObFYF7xeKFJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Our entire pipeline collapses into a three lines\n",
        "dataset_uri = 'hf://datasets/HuggingFaceM4/the_cauldron/ai2d/train-00000-of-00001-2ce340398c113b79.parquet'\n",
        "pipeline = TheCauldronImageUnderstandingEvaluationPipeline(api_key = \"none\", base_url = \"http://0.0.0.0:8000/v1\")\n",
        "df = pipeline(model_id = 'google/gemma-3n-e4b-it', sampling_params={\"temperature\": 0.1}, is_eager=True)"
      ],
      "metadata": {
        "id": "A98SLoXALhxR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Materialize if not eager\n",
        "df_mat = df.collect()"
      ],
      "metadata": {
        "id": "mFdEC8JDMj8E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the Pass/Fail Rate\n",
        "print(f\"Pass/Fail Rate: {pipeline.evaluate(df_mat)}\""
      ],
      "metadata": {
        "id": "0v6x19XxNyJa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Conclusion\n",
        "\n",
        "In this notebook we explored how to evaluate Gemma-3's image understanding using a subset from HuggingFace's TheCauldron Dataset. The AI2D subset we used is just one of a massive collection of 50 vision-language datasets that can be used for evaluating or training vision language models totaling millions of rows.\n",
        "\n",
        "A natural next step would be to parallelize this pipeline across multiple datasets leveraging multiple gpus. In this scenario, I recommend transitioning daft's execution context to leverage Ray, a distributed compute framework built in natively.\n",
        "\n",
        "```bash\n",
        "pip install \"daft[huggingface,ray]\"\n",
        "```\n",
        "\n",
        "You can set daft's execution context to ray adding the `ray` optional dependency during installation and running the following at the top of your script.\n",
        "\n",
        "\n",
        "```python\n",
        "import daft\n",
        "\n",
        "daft.set_runner_ray()\n",
        "```\n",
        "\n",
        "Simply run your pipeline across each dataset uri and collect the results. Daft will orchestrate ray in the background for you.\n",
        "\n",
        "You could also leverage this pipeline to evaluate model performance across sampling parameters or model variants. Please note that not all Gemma-3 series models support image inputs, and leveraging datasets outside of the TheCauldron would require different preprocessing stages."
      ],
      "metadata": {
        "id": "VbzOoC62uDu1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Appendix\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "J_wIalJJs0ki"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From Gemma Model Card"
      ],
      "metadata": {
        "id": "HYsTmBJ74p1w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoProcessor, Gemma3nForConditionalGeneration\n",
        "from PIL import Image\n",
        "import requests\n",
        "import torch\n",
        "\n",
        "model_id = \"google/gemma-3n-e4b-it\"\n",
        "model = Gemma3nForConditionalGeneration.from_pretrained(model_id, device_map=\"auto\", torch_dtype=torch.bfloat16,).eval()\n",
        "processor = AutoProcessor.from_pretrained(model_id)\n",
        "\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": [{\"type\": \"text\", \"text\": \"You are a helpful assistant.\"}]\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": [\n",
        "            {\"type\": \"image\", \"image\": \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/bee.jpg\"},\n",
        "            {\"type\": \"text\", \"text\": \"Describe this image in detail.\"}\n",
        "        ]\n",
        "    }\n",
        "]\n",
        "\n",
        "inputs = processor.apply_chat_template(\n",
        "    messages,\n",
        "    add_generation_prompt=True,\n",
        "    tokenize=True,\n",
        "    return_dict=True,\n",
        "    return_tensors=\"pt\",\n",
        ").to(model.device)\n",
        "\n",
        "input_len = inputs[\"input_ids\"].shape[-1]\n",
        "\n",
        "with torch.inference_mode():\n",
        "    generation = model.generate(**inputs, max_new_tokens=100, do_sample=False)\n",
        "    generation = generation[0][input_len:]\n",
        "\n",
        "decoded = processor.decode(generation, skip_special_tokens=True)\n",
        "print(decoded)"
      ],
      "metadata": {
        "id": "hwDJVWrufZkp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "vllm usage patterns for multimodal."
      ],
      "metadata": {
        "id": "3O5YYKLbZlfz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "# Multi-image input inference\n",
        "def run_multi_image(model: str) -> None:\n",
        "    image_url_duck = \"https://upload.wikimedia.org/wikipedia/commons/d/da/2015_Kaczka_krzy%C5%BCowka_w_wodzie_%28samiec%29.jpg\"\n",
        "    image_url_lion = \"https://upload.wikimedia.org/wikipedia/commons/7/77/002_The_lion_king_Snyggve_in_the_Serengeti_National_Park_Photo_by_Giles_Laurent.jpg\"\n",
        "    chat_completion_from_url = client.chat.completions.create(\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": [\n",
        "                    {\"type\": \"text\", \"text\": \"What are the animals in these images?\"},\n",
        "                    {\n",
        "                        \"type\": \"image_url\",\n",
        "                        \"image_url\": {\"url\": image_url_duck},\n",
        "                    },\n",
        "                    {\n",
        "                        \"type\": \"image_url\",\n",
        "                        \"image_url\": {\"url\": image_url_lion},\n",
        "                    },\n",
        "                ],\n",
        "            }\n",
        "        ],\n",
        "        model=model,\n",
        "        max_completion_tokens=64,\n",
        "    )\n",
        "\n",
        "    result = chat_completion_from_url.choices[0].message.content\n",
        "    print(\"Chat completion output:\", result)\n",
        "\n",
        "\n",
        "# Video input inference\n",
        "def run_video(model: str) -> None:\n",
        "    video_url = \"http://commondatastorage.googleapis.com/gtv-videos-bucket/sample/ForBiggerFun.mp4\"\n",
        "    video_base64 = encode_base64_content_from_url(video_url)\n",
        "\n",
        "    ## Use video url in the payload\n",
        "    chat_completion_from_url = client.chat.completions.create(\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": [\n",
        "                    {\"type\": \"text\", \"text\": \"What's in this video?\"},\n",
        "                    {\n",
        "                        \"type\": \"video_url\",\n",
        "                        \"video_url\": {\"url\": video_url},\n",
        "                    },\n",
        "                ],\n",
        "            }\n",
        "        ],\n",
        "        model=model,\n",
        "        max_completion_tokens=64,\n",
        "    )\n",
        "\n",
        "    result = chat_completion_from_url.choices[0].message.content\n",
        "    print(\"Chat completion output from image url:\", result)\n",
        "\n",
        "    ## Use base64 encoded video in the payload\n",
        "    chat_completion_from_base64 = client.chat.completions.create(\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": [\n",
        "                    {\"type\": \"text\", \"text\": \"What's in this video?\"},\n",
        "                    {\n",
        "                        \"type\": \"video_url\",\n",
        "                        \"video_url\": {\"url\": f\"data:video/mp4;base64,{video_base64}\"},\n",
        "                    },\n",
        "                ],\n",
        "            }\n",
        "        ],\n",
        "        model=model,\n",
        "        max_completion_tokens=64,\n",
        "    )\n",
        "\n",
        "    result = chat_completion_from_base64.choices[0].message.content\n",
        "    print(\"Chat completion output from base64 encoded image:\", result)\n",
        "\n",
        "\n",
        "# Audio input inference\n",
        "def run_audio(model: str) -> None:\n",
        "    from vllm.assets.audio import AudioAsset\n",
        "\n",
        "    audio_url = AudioAsset(\"winning_call\").url\n",
        "    audio_base64 = encode_base64_content_from_url(audio_url)\n",
        "\n",
        "    # OpenAI-compatible schema (`input_audio`)\n",
        "    chat_completion_from_base64 = client.chat.completions.create(\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": [\n",
        "                    {\"type\": \"text\", \"text\": \"What's in this audio?\"},\n",
        "                    {\n",
        "                        \"type\": \"input_audio\",\n",
        "                        \"input_audio\": {\n",
        "                            # Any format supported by librosa is supported\n",
        "                            \"data\": audio_base64,\n",
        "                            \"format\": \"wav\",\n",
        "                        },\n",
        "                    },\n",
        "                ],\n",
        "            }\n",
        "        ],\n",
        "        model=model,\n",
        "        max_completion_tokens=64,\n",
        "    )\n",
        "\n",
        "    result = chat_completion_from_base64.choices[0].message.content\n",
        "    print(\"Chat completion output from input audio:\", result)\n",
        "\n",
        "    # HTTP URL\n",
        "    chat_completion_from_url = client.chat.completions.create(\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": [\n",
        "                    {\"type\": \"text\", \"text\": \"What's in this audio?\"},\n",
        "                    {\n",
        "                        \"type\": \"audio_url\",\n",
        "                        \"audio_url\": {\n",
        "                            # Any format supported by librosa is supported\n",
        "                            \"url\": audio_url\n",
        "                        },\n",
        "                    },\n",
        "                ],\n",
        "            }\n",
        "        ],\n",
        "        model=model,\n",
        "        max_completion_tokens=64,\n",
        "    )\n",
        "\n",
        "    result = chat_completion_from_url.choices[0].message.content\n",
        "    print(\"Chat completion output from audio url:\", result)\n",
        "\n",
        "    # base64 URL\n",
        "    chat_completion_from_base64 = client.chat.completions.create(\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": [\n",
        "                    {\"type\": \"text\", \"text\": \"What's in this audio?\"},\n",
        "                    {\n",
        "                        \"type\": \"audio_url\",\n",
        "                        \"audio_url\": {\n",
        "                            # Any format supported by librosa is supported\n",
        "                            \"url\": f\"data:audio/ogg;base64,{audio_base64}\"\n",
        "                        },\n",
        "                    },\n",
        "                ],\n",
        "            }\n",
        "        ],\n",
        "        model=model,\n",
        "        max_completion_tokens=64,\n",
        "    )\n",
        "\n",
        "    result = chat_completion_from_base64.choices[0].message.content\n",
        "    print(\"Chat completion output from base64 encoded audio:\", result)\n"
      ],
      "metadata": {
        "id": "1sgl3wEyZkCn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FWMccfnu-Qdy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3500438b"
      },
      "source": [
        "import base64\n",
        "\n",
        "# Example image bytes (first few bytes of a PNG)\n",
        "image_bytes = b\"\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHD\"\n",
        "\n",
        "# Encode the bytes to base64\n",
        "base64_string = base64.b64encode(image_bytes).decode('utf-8')\n",
        "\n",
        "print(base64_string)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cc5b2066"
      },
      "source": [
        "In this code:\n",
        "- We import the `base64` module.\n",
        "- We have a sample `image_bytes` byte string.\n",
        "- `base64.b64encode(image_bytes)` encodes the byte string into a base64 byte string.\n",
        "- `.decode('utf-8')` decodes the base64 byte string into a UTF-8 string, which is the standard representation for base64."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import daft\n",
        "import base64\n",
        "\n",
        "df_raw = daft.read_parquet('hf://datasets/HuggingFaceM4/the_cauldron/ai2d/train-00000-of-00001-2ce340398c113b79.parquet')\n",
        "\n",
        "# To Get Daft Image\n",
        "df = df_raw.explode(col(\"images\")).with_column(\"image_png\", df[\"images\"].struct.get(\"bytes\").image.decode())\n",
        "\n",
        "# To Get Base64\n",
        "df = df.with_column(\n",
        "    \"image_base64\", df[\"images\"].struct.get(\"bytes\").apply(\n",
        "        lambda x: base64.b64encode(x).decode('utf-8'),\n",
        "        return_dtype=daft.DataType.string()\n",
        "    )\n",
        ").collect()\n",
        "df.show()\n"
      ],
      "metadata": {
        "id": "tAyaykz3EURx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!lscpu"
      ],
      "metadata": {
        "id": "V2hqlqJZEUh6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Nz8gto9vP_fS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}